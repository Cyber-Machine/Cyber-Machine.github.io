{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/brief-intro-to-reinforcement-learning","result":{"data":{"post":{"slug":"/brief-intro-to-reinforcement-learning","title":"Brief intro to Reinforcement Learning.","date":"08.10.2022","tags":[{"name":"Deep Learning","slug":"deep-learning"},{"name":"Reinforcement Learning","slug":"reinforcement-learning"},{"name":"Mathematics","slug":"mathematics"}],"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Brief intro to Reinforcement Learning.\",\n  \"date\": \"2022-10-08T00:00:00.000Z\",\n  \"tags\": [\"Deep Learning\", \"Reinforcement Learning\", \"Mathematics\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Artificial Intelligence has been evolving quite a lot in recent years, and there are a lot of breakthroughs that are happening since the inception of Deep learning, surplus of data and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.researchgate.net/publication/325023664_Performance_of_CPUsGPUs_for_Deep_Learning_workloads\"\n  }, \"enhanced hardware (GPU's / TPU's)\"), \" & \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\"\n  }, \"acclerated programming\"), \".\"), mdx(\"p\", null, \"Currently AI has achieved a significant amount of things such as \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.deepmind.com/research/highlighted-research/alphago\"\n  }, \"beating human in games\"), \", \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://cdn.openai.com/papers/whisper.pdf\"\n  }, \"human-level speech recognition\"), \" towards \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://ai.googleblog.com/2022/07/mlgo-machine-learning-framework-for.html\"\n  }, \"optimizing compilers\"), \", and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.nature.com/articles/s41586-022-05172-4\"\n  }, \"developing algorithms\"), \".\"), mdx(\"p\", null, \"Reinforcement Learning is one of the driving force that led to this innovations the past few years.\"), mdx(\"h2\", null, \"Reinforcement Learning\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Reinforcement learning problems involve learning what to do\\u2014how to map situations to actions\\u2014so as to maximize a numerical reward signal. In\\nan essential way they are closed-loop problems because the learning system\\u2019s actions influence its later inputs. \"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\"\n  }, \"Richard S. Sutton and Andrew G. Barto\"))), mdx(\"p\", null, \"When I think of Reinforcement learning, I think about a space video game where we have the controls for our space guns and we have to shoot asteriods in space with limited ammunation.\\nIf we try to destroy all asteroids in space our ammo will run out and we won't be able to shoot nearby asteriod and that will lead to untimely demise of our ship and same thing would happen if we save too much ammo by not shooting asteriod.\\nSince it is a simple game we can try multiple turns and learn an optimal way of saving ourselves from the asteroids.\"), mdx(\"p\", null, \"We can apply A.I. (Reinforcement learning) to save ourselves from these attacks by training our model to learn the enough trail and error.\\nIn Reinforcement Learning we have:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Agent\"), \" - Our Spaceship\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Environment\"), \" - Space\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"State\"), \" - A part of environment seen by our spaceship\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Action\"), \" - To shoot or not \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Reward\"), \" - Weather we are alive or not\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Policy\"), \" - The continuous series of action till we end the game\")), mdx(\"p\", null, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"908px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"38.75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAIAAAB2/0i6AAAACXBIWXMAAAsTAAALEwEAmpwYAAABK0lEQVQY023Q247CIBAGYN//vYwmNVIxwVZqgA5Uu4RsLK0aDrKx6O7NfheEZJjJ/CzSLIQQY8xnjNE5p7X+nhljYozpP4uUklKKENI0DaX0eDwyxpbL5Wq1QgiVZVlV1eFwAAAx45x3Xfd8Pt/NQoi2bR+Ph53dbjdjDMZ4u93udjtK6TRNeS/vfUqJMeacezUTQjDGlNKqqk6nUx6ZUvLeu1luyHI1P+77fjGOoxCCEDJNUw4fQhiGwVo7fgzDkEfk8IQQzrn3/rU2AJzP5xCCMUZKKYTouu7rQ2t9uVyklH6WUmqa5n6/vzMDQF3XAFAURVmWCKG6rhFCm81mv9+v12tKaVEUGGMAYIwppf5+2zl3vV6ttVrrnKJtWzkDAKUU57zv+9/Mv5cfh3C7gNjBH3MAAAAASUVORK5CYII=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Rl\",\n    \"title\": \"Rl\",\n    \"src\": \"/static/98f1a4f564cd36fa778b8fbd245803df/68052/Rl.png\",\n    \"srcSet\": [\"/static/98f1a4f564cd36fa778b8fbd245803df/5243c/Rl.png 240w\", \"/static/98f1a4f564cd36fa778b8fbd245803df/ab158/Rl.png 480w\", \"/static/98f1a4f564cd36fa778b8fbd245803df/68052/Rl.png 908w\"],\n    \"sizes\": \"(max-width: 908px) 100vw, 908px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \"), \"\\n\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://optimization.cbe.cornell.edu/index.php?title=Main_Page\"\n  }, \"Reinforcement learning\")), mdx(\"p\", null, \"Together these forms the basis for Reinforcement Learning.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Reinforcement learning tries to learn a optimal policy through trail-and-error by maximizing rewards through actions in a particular environment.\")), mdx(\"p\", null, \"In order to build an optimal policy, the agent faces the dilemma of exploring new states while maximizing its overall reward at the same time. This is called \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://sites.cs.ucsb.edu/~suri/ccs130a/MultiArmBandit.pdf\"\n  }, \"Exploration vs Exploitation trade-off\"), \". To balance both, the best overall strategy may involve short term sacrifices. Therefore, the agent should collect enough information to make the best overall decision in the future.\"), mdx(\"p\", null, \"What I personally like about Reinforcement Learning is how it ties up mathematics and human nature.\"), mdx(\"p\", null, \"We humans learn a lot through experimentation and challenges, and we use those experiences to better suit us to our environment.\\nWe are constantly evolving and exploring our environment to overcome adversaries.\"), mdx(\"p\", null, \"While I was learning how to solve these problems I came up with a lot of things in field of probabilities and decision making such as Markov chain, Dynamic Programming, Monte-Carlo techniques and so on. I was intrigued by the fact that how these functions help our model to form better decision over time. \"), mdx(\"h2\", null, \"Techniques Used in Reinforcment Learning\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://towardsdatascience.com/introduction-to-markov-chains-50da3645a50d\"\n  }, \"Markov Chains\")), mdx(\"p\", null, \"Markov chains are a fairly common, and relatively simple, way to statistically model random processes. It consists of a state of transistions which are determined by some probability distributions that satisfy the Markov Property.\"), mdx(\"p\", null, \"It basically generates a probability distribution on the basis of data gathered and uses it for prediction.\"), mdx(\"h3\", null, mdx(\"a\", {\n    parentName: \"h3\",\n    \"href\": \"https://en.wikipedia.org/wiki/Q-learning\"\n  }, \"Q-learning\")), mdx(\"p\", null, \"Q-learning is a off-policy, model free algorithm that updates \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"http://www.scholarpedia.org/article/Temporal_difference_learning#:~:text=Temporal%20difference%20(TD)%20learning%20is,to%20drive%20the%20learning%20process.\"\n  }, \"it's value at the end of each step instead of at the end of an episode\"), \".\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Q-Learning is the algorithm we use to train our Q-function, an action-value function that determines the value of being at a particular state and taking a specific action at that state.\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Q comes from \\u201Cthe Quality\\u201D (the value) of that action at that state.\")), mdx(\"p\", null, \"Q learning is not applicable for huge data where an actor can perform multiple actions, this is because as the number of actions increases the space complexity of this algorithm increases polynomial times which makes our incapable to use.\"), mdx(\"h2\", null, \"Recent Advancements\"), mdx(\"h3\", null, \"Deep Learning\"), mdx(\"p\", null, \"Deep Learning is a subset of machine learning inspired by neuron's present in brain. It gained popularity around 2012 during \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/1409.0575.pdf\"\n  }, \"ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\"), \", The ILSVRC is an annual computer vision competition developed upon a subset of a publicly available computer vision dataset called ImageNet. As such, the tasks and even the challenge itself is often referred to as the ImageNet Competition.\\nThe goal of the challenge was to both promote the development of better computer vision techniques and to benchmark the state of the art.\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\"\n  }, \"AlexNet\"), \" was developed using a deep convolutional neural network, which achieved excellent results in the classification of images. There has been a steady rise in the popularity of deep learning techniques since then.\"), mdx(\"p\", null, \"Deep learning has made significant progress since then and is now used in a wide variety of applications, one the the key example lies with \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://alphafold.ebi.ac.uk/\"\n  }, \"AlphaFold\"), \" which was \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://predictioncenter.org/casp14/zscores_final.cgi\"\n  }, \"the top-ranked protein structure prediction method by a large margin, producing predictions with high accuracy\"), \" which will help us to find better solutions in finding.\"), mdx(\"h3\", null, \"Accelerated Programming\"), mdx(\"p\", null, \"There are many frameworks that evolved recently that uses parallel programming and efficient use of resources in order to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://escholarship.org/uc/item/84w2k2xr\"\n  }, \"speed up the process\"), \". These frameworks helped a lot in Machine Learning research and allow us to access bigger, distributed data and train models on them.\"), mdx(\"h2\", null, \"Deep-Q-Learning\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Deep Q-Learning uses a deep neural network to approximate the different Q-values for each possible action at a state.\"), mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"55.00000000000001%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/webp;base64,UklGRnAAAABXRUJQVlA4IGQAAABwAwCdASoUAAsAPtFUo0uoJKMhsAgBABoJZwAAYfIPUSUhQAAA/vPH85t678LhFij8xUbe08fVLPopnnHFsGdYlFCAYrZncRa36qAOf1yXZD+kMkh10gEqx1lhJuPrmfL58AAA')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Deep-RL\",\n    \"title\": \"Deep-RL\",\n    \"src\": \"/static/6b0e91b910854849e7bd5203584617ed/10c02/Deep-RL.webp\",\n    \"srcSet\": [\"/static/6b0e91b910854849e7bd5203584617ed/cbea2/Deep-RL.webp 240w\", \"/static/6b0e91b910854849e7bd5203584617ed/42669/Deep-RL.webp 480w\", \"/static/6b0e91b910854849e7bd5203584617ed/10c02/Deep-RL.webp 960w\", \"/static/6b0e91b910854849e7bd5203584617ed/20693/Deep-RL.webp 1396w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n    \"))), mdx(\"p\", null, \"Further Reading :\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://incompleteideas.net/book/ebook/node12.html\"\n  }, \"History of Reinforcement Learning\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://lilianweng.github.io/posts/2018-02-19-rl-overview/#sarsa-on-policy-td-control\"\n  }, \"lilianweng\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://computerhistory.org/blog/ai-and-play-part-2-go-and-deep-learning/\"\n  }, \"Computer History Museum\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/cs/9605103.pdf\"\n  }, \"Survey Paper\"))));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"Artificial Intelligence has been evolving quite a lot in recent years, and there are a lot of breakthroughs that are happening since the…","timeToRead":3,"banner":null}},"pageContext":{"slug":"/brief-intro-to-reinforcement-learning","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["2744905544","3090400250","318001574"]}