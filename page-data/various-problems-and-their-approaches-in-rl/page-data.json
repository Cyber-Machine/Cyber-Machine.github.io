{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/various-problems-and-their-approaches-in-rl","result":{"data":{"post":{"slug":"/various-problems-and-their-approaches-in-rl","title":"Various problems and their approaches in RL.","date":"08.10.2022","tags":[{"name":"Machine Learning","slug":"machine-learning"},{"name":"Reinforcement Learning","slug":"reinforcement-learning"},{"name":"Deep Learning","slug":"deep-learning"}],"description":null,"canonicalUrl":null,"body":"var _excluded = [\"components\"];\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Various problems and their approaches in RL.\",\n  \"date\": \"2022-10-08T00:00:00.000Z\",\n  \"tags\": [\"Machine Learning\", \"Reinforcement Learning\", \"Deep Learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"p\", null, \"Recently, I have been into Learning Reinforcement Learning, it is a field of Machine Learning where a model is learns through (decision making)/\\n(maximizing rewards through action)/(interactive)/(game playing)\"), mdx(\"h2\", null, \"Multi-Armed Bandits\"), mdx(\"p\", null, \"Suppose you are an novice chess player, each move you take can result you in either winning or losing the game. Through repeated games you gain experience and then use it to maximize your chances in winning the game. This similiar analogy is applied to k-armed bandits problem where we try to maximize our liklihood of winning our of k options to maximize rewards or values.\"), mdx(\"p\", null, \"In an environment \"), mdx(\"center\", null, \"q\\u21E4 (a) = E[Rt | At = a]\"), mdx(\"h4\", null, \"Incremental Implementation\"), mdx(\"p\", null, \"In order to compute averages in a computationally efficient manner \"), mdx(\"h4\", null, \"Upper Confidence Bound (UCB)\"), mdx(\"p\", null, \"The idea of this upper confidence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a\\u2019s value.\"), mdx(\"p\", null, \"/// Equation and its meaning \"), mdx(\"p\", null, \"####Gradient Bandit Algorithms\\nwe consider learning a numerical preference for each action ,The larger the preference, the more often that action is\\ntaken, but the preference has no interpretation in terms of reward. Only the relative\\npreference of one action over another is important; if we add 1000 to all the action\\npreferences there is no effect on the action probabilities, which are determined according\\nto a soft-max distribution\"), mdx(\"p\", null, \"####Contextual Bandits\"), mdx(\"h2\", null, \"Finite Markove Decision\"), mdx(\"p\", null, \"MDPs are a classical formalization of sequential decision making,where actions influence not just immediate rewards, but also subsequent situations,or states, and through those future rewards. Thus MDPs involve delayed reward and the need to trade off immediate and delayed reward\"));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":"Recently, I have been into Learning Reinforcement Learning, it is a field of Machine Learning where a model is learns through (decisionâ€¦","timeToRead":1,"banner":null}},"pageContext":{"slug":"/various-problems-and-their-approaches-in-rl","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["2744905544","3090400250","318001574"]}